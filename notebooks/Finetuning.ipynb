{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning moondream\n",
    "\n",
    "This notebook demonstrates how to fine-tune moondream to improve performance on a downstream task.\n",
    "For this example, we'll fine-tune on this [Captcha image dataset](https://huggingface.co/datasets/project-sloth/captcha-images).\n",
    "\n",
    "The first step is to construct a dataset split into `train`, `validation`, and `test` sets. This is\n",
    "not strictly necessary and can be skipped if you're fine-tuning on a very small amount of data. We\n",
    "will train the model on the `train` set, use the `validation` set to tune hyperparameters and prevent\n",
    "overfitting, and finally evaluate the model on the `test` set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch transformers timm einops datasets bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class CaptchaDataset(Dataset):\n",
    "    def __init__(self, split=\"train\"):\n",
    "        self.data = load_dataset(\n",
    "            \"project-sloth/captcha-images\",\n",
    "            revision=\"refs/convert/parquet\",\n",
    "        )[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return {\n",
    "            \"image\": sample[\"image\"],  # Should be a PIL image\n",
    "            \"qa\": [\n",
    "                {\n",
    "                    \"question\": \"What does the text say?\",\n",
    "                    \"answer\": sample[\"solution\"],\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "\n",
    "\n",
    "datasets = {\n",
    "    \"train\": CaptchaDataset(\"train\"),\n",
    "    \"val\": CaptchaDataset(\"validation\"),\n",
    "    \"test\": CaptchaDataset(\"test\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at a sample image from the training set and compare the ground-truth answers\n",
    "with moondream predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize moondream. Change DEVICE to 'mps' if you're on an M1 Mac, or 'cpu' if you don't have a\n",
    "# GPU. Note that fine-tuning on CPU will be very slow.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "DEVICE = \"cuda\"\n",
    "DTYPE = (\n",
    "    torch.float32 if DEVICE == \"cpu\" else torch.float16\n",
    ")  # CPU doesn't support float16\n",
    "MD_REVISION = \"2024-04-02\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vikhyatk/moondream2\", revision=MD_REVISION)\n",
    "moondream = AutoModelForCausalLM.from_pretrained(\n",
    "    \"vikhyatk/moondream2\",\n",
    "    revision=MD_REVISION,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if DEVICE == \"cuda\" else None,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map={\"\": DEVICE},\n",
    "    # device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moondream.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "sample = datasets[\"train\"][0]\n",
    "display(sample[\"image\"])\n",
    "\n",
    "for qa in sample[\"qa\"]:\n",
    "    print(\"Question:\", qa[\"question\"])\n",
    "    print(\"Ground Truth:\", qa[\"answer\"])\n",
    "    print(\n",
    "        \"Moondream:\",\n",
    "        moondream.answer_question(\n",
    "            moondream.encode_image(sample[\"image\"]),\n",
    "            qa[\"question\"],\n",
    "            tokenizer=tokenizer,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start setting up hyperparameters for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of times to repeat the training dataset. Increasing this may cause the model to overfit or\n",
    "# lose generalization due to catastrophic forgetting. Decreasing it may cause the model to underfit.\n",
    "EPOCHS = 2\n",
    "\n",
    "# Number of samples to process in each batch. Set this to the highest value that doesn't cause an\n",
    "# out-of-memory error. Decrease it if you're running out of memory. Batch size 8 currently uses around\n",
    "# 15 GB of GPU memory during fine-tuning.\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Number of batches to process before updating the model. You can use this to simulate a higher batch\n",
    "# size than your GPU can handle. Set this to 1 to disable gradient accumulation.\n",
    "GRAD_ACCUM_STEPS = 1\n",
    "\n",
    "# Learning rate for the Adam optimizer. Needs to be tuned on a case-by-case basis. As a general rule\n",
    "# of thumb, increase it by 1.4 times each time you double the effective batch size.\n",
    "#\n",
    "# Source: https://www.cs.princeton.edu/~smalladi/blog/2024/01/22/SDEs-ScalingRules/\n",
    "#\n",
    "# Note that we linearly warm the learning rate up from 0.1 * LR to LR over the first 10% of the\n",
    "# training run, and then decay it back to 0.1 * LR over the last 90% of the training run using a\n",
    "# cosine schedule.\n",
    "LR = 3e-5\n",
    "\n",
    "# Whether to use Weights and Biases for logging training metrics.\n",
    "USE_WANDB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block will start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from bitsandbytes.optim import Adam8bit\n",
    "import math\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "\n",
    "ANSWER_EOS = \"<|endoftext|>\"\n",
    "\n",
    "# Number of tokens used to represent each image.\n",
    "IMG_TOKENS = 729\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [sample[\"image\"] for sample in batch]\n",
    "    images = torch.stack(moondream.vision_encoder.preprocess(images))\n",
    "    images = rearrange(images, \"b c (h p1) (w p2) -> b (h w) (c p1 p2)\", p1=14, p2=14)\n",
    "\n",
    "    labels_acc = []\n",
    "    tokens_acc = []\n",
    "\n",
    "    for sample in batch:\n",
    "        toks = [tokenizer.bos_token_id]\n",
    "        labs = [-100] * (IMG_TOKENS + 1)\n",
    "\n",
    "        for qa in sample[\"qa\"]:\n",
    "            q_t = tokenizer(\n",
    "                f\"\\n\\nQuestion: {qa['question']}\\n\\nAnswer:\", add_special_tokens=False\n",
    "            ).input_ids\n",
    "            toks.extend(q_t)\n",
    "            labs.extend([-100] * len(q_t))\n",
    "\n",
    "            a_t = tokenizer(\n",
    "                f\" {qa['answer']}{ANSWER_EOS}\", add_special_tokens=False\n",
    "            ).input_ids\n",
    "            toks.extend(a_t)\n",
    "            labs.extend(a_t)\n",
    "\n",
    "        tokens_acc.append(toks)\n",
    "        labels_acc.append(labs)\n",
    "\n",
    "    max_len = -1\n",
    "    for labels in labels_acc:\n",
    "        max_len = max(max_len, len(labels))\n",
    "\n",
    "    attn_mask_acc = []\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        len_i = len(labels_acc[i])\n",
    "        pad_i = max_len - len_i\n",
    "\n",
    "        labels_acc[i].extend([-100] * pad_i)\n",
    "        tokens_acc[i].extend([tokenizer.eos_token_id] * pad_i)\n",
    "        attn_mask_acc.append([1] * len_i + [0] * pad_i)\n",
    "\n",
    "    return (\n",
    "        images.to(dtype=DTYPE),\n",
    "        torch.stack([torch.tensor(t, dtype=torch.long) for t in tokens_acc]),\n",
    "        torch.stack([torch.tensor(l, dtype=torch.long) for l in labels_acc]),\n",
    "        torch.stack([torch.tensor(a, dtype=torch.bool) for a in attn_mask_acc]),\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_loss(batch):\n",
    "    images, tokens, labels, attn_mask = batch\n",
    "\n",
    "    images = images.to(DEVICE)\n",
    "    tokens = tokens.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    attn_mask = attn_mask.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_embs = moondream.vision_encoder.encoder(images)\n",
    "        img_embs = moondream.vision_encoder.projection(img_embs)\n",
    "\n",
    "    tok_embs = moondream.text_model.get_input_embeddings()(tokens)\n",
    "    inputs_embeds = torch.cat(\n",
    "        (tok_embs[:, 0:1, :], img_embs, tok_embs[:, 1:, :]), dim=1\n",
    "    )\n",
    "\n",
    "    outputs = moondream.text_model(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        labels=labels,\n",
    "        attention_mask=attn_mask,\n",
    "    )\n",
    "\n",
    "    return outputs.loss\n",
    "\n",
    "\n",
    "def lr_schedule(step, max_steps):\n",
    "    x = step / max_steps\n",
    "    if x < 0.1:\n",
    "        return 0.1 * LR + 0.9 * LR * x / 0.1\n",
    "    else:\n",
    "        return 0.1 * LR + 0.9 * LR * (1 + math.cos(math.pi * (x - 0.1))) / 2\n",
    "\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": DataLoader(\n",
    "        datasets[\"train\"],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "    ),\n",
    "    \"val\": DataLoader(\n",
    "        datasets[\"val\"],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        collate_fn=collate_fn,\n",
    "    ),\n",
    "}\n",
    "\n",
    "moondream.text_model.train()\n",
    "moondream.text_model.transformer.gradient_checkpointing_enable()\n",
    "\n",
    "total_steps = EPOCHS * len(dataloaders[\"train\"]) // GRAD_ACCUM_STEPS\n",
    "optimizer = Adam8bit(\n",
    "    [\n",
    "        {\"params\": moondream.text_model.parameters()},\n",
    "    ],\n",
    "    lr=LR * 0.1,\n",
    "    betas=(0.9, 0.95),\n",
    "    eps=1e-6,\n",
    ")\n",
    "\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"moondream-ft\",\n",
    "        config={\n",
    "            \"EPOCHS\": EPOCHS,\n",
    "            \"BATCH_SIZE\": BATCH_SIZE,\n",
    "            \"GRAD_ACCUM_STEPS\": GRAD_ACCUM_STEPS,\n",
    "            \"LR\": LR,\n",
    "        },\n",
    "    )\n",
    "\n",
    "i = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in tqdm(dataloaders[\"train\"], desc=f\"Epoch {epoch + 1}/{EPOCHS}\"):\n",
    "        i += 1\n",
    "\n",
    "        loss = compute_loss(batch)\n",
    "        loss.backward()\n",
    "\n",
    "        if i % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            lr = lr_schedule(i / GRAD_ACCUM_STEPS, total_steps)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = lr\n",
    "\n",
    "        if i % 100 == 0 and USE_WANDB:\n",
    "            # Calculate validation loss\n",
    "            val_loss = 0\n",
    "            for val_batch in tqdm(dataloaders[\"val\"], desc=\"Validation\"):\n",
    "                with torch.no_grad():\n",
    "                    val_loss += compute_loss(val_batch).item()\n",
    "            val_loss /= len(dataloaders[\"val\"])\n",
    "\n",
    "        if USE_WANDB:\n",
    "            wandb.log(\n",
    "                {\"loss/train\": loss.item(), \"lr\": optimizer.param_groups[0][\"lr\"]}\n",
    "                | ({\"loss/val\": val_loss} if i % 100 == 0 else {})\n",
    "            )\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moondream.save_pretrained(\"checkpoints/moondream-ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training has completed, let's inspect a few samples and calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moondream.eval()\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for i, sample in enumerate(datasets[\"test\"]):\n",
    "    md_answer = moondream.answer_question(\n",
    "        moondream.encode_image(sample[\"image\"]),\n",
    "        sample[\"qa\"][0][\"question\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    if md_answer == sample[\"qa\"][0][\"answer\"]:\n",
    "        correct += 1\n",
    "\n",
    "    if i < 3:\n",
    "        display(sample[\"image\"])\n",
    "        print(\"Question:\", sample[\"qa\"][0][\"question\"])\n",
    "        print(\"Ground Truth:\", sample[\"qa\"][0][\"answer\"])\n",
    "        print(\"Moondream:\", md_answer)\n",
    "\n",
    "print(f\"\\n\\nAccuracy: {correct / len(datasets['test']) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
